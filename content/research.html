<h1 class="entry-title">Research</h1>

<p>SELF-ORGANIZATION, SPREADING DYNAMICS &amp; INFORMATION PROCESSING</p>

<p>The human brain proves amazing information processing capacities, which rely on the coordinated activity of 80 billion neurons; each neuron interacting with thousands of other neurons. I want to understand how these neurons jointly form a functional network for information processing. Given that trillions of neural connections (synapses) have to be coordinated to perform meaningful information processing, this &#8220;infogenesis&#8221; must be primarily driven by self-organizing principles. A key role is dedicated to the self-organization through local learning rules, thus principles that update the connection strength between neurons only based on locally available information. My aim is to derive from first principles how such local learning rules can optimize a network for information processing &#8211; ideally even before ever being exposed to specific stimuli of the external world. If identified, such a self-organization mechanism on the one hand can improve the initialization and effectiveness of artificial neural networks, and on the other hand shed light on the riddle of how our complex brains cope so seemingly effortless with the complex environment.</p>
<p>PROJECT DETAILS</p>
<p><strong>I. Subsampling. </strong>In order to understand living information processing, we need to sample neural activity from the brain. However, in the brain like in most large systems, it is impossible to sample the activity of all units in parallel. The human brain for example comprises 80 billion neurons, but current techniques allow sampling the activity from only a few thousand neurons at a time. How can we infer properties from the full system if only observing a tiny fraction? Unfortunately, inferences are at times not straight forward, but systematically biased. For example, subsampling in critical models can distort the expected power law relations, and thereby a critical system can be misinterpreted as sub- or super-critical. We are developing approaches to overcome subsampling effects by extending methods from finite size scaling theory, employing stochastic processes and harnessing the theory of non-equilibrium phase transitions. For example, we know know how to infer spreading dynamics even if only a tiny fraction of all neurons is sampled &#8212; under idealized condition sampling a single unit would be sufficient to estimate the spreading rate!</p>
<p>Selected References:<br />
VP et al., <em>Frontiers in Systems Neuroscience (</em>2014)<br />
Levina &amp; VP, <em>Nature Communications</em> (2017)<br />
Wilting &amp;VP, <em>Nature Communications</em> (2018)<br />
Neto, Spitzner &amp; VP, arxiv</p>
<figure id="attachment_113" style="width: 294px" class="wp-caption aligncenter"><img class="wp-image-113 " src="http://www.viola-priesemann.de/wp-content/uploads/2017/09/Subsampling1_cropped-300x226.jpg" alt="" width="294" height="221" srcset="https://www.viola-priesemann.de/wp-content/uploads/2017/09/Subsampling1_cropped-300x226.jpg 300w, https://www.viola-priesemann.de/wp-content/uploads/2017/09/Subsampling1_cropped-768x578.jpg 768w, https://www.viola-priesemann.de/wp-content/uploads/2017/09/Subsampling1_cropped.jpg 1021w" sizes="(max-width: 294px) 100vw, 294px" /><figcaption class="wp-caption-text">In experiments only a tiny fraction (right) of all neurons can be sampled in parallel (figure from Wilting &amp; VP, 2018, generated with <a href="http://www.treestoolbox.org" target="_blank" rel="noopener">TREES</a>).</figcaption></figure>
<p>&nbsp;</p>
<p><strong>II. Reverberating, Subcritical Dynamics in vivo.</strong> In neuroscience, a popular hypothesis states that the collective neural dynamics should self-organize to a critical state, because at criticality simple models maximize their information processing capacities. However, criticality may also come with the risk of spontaneous runaway activity (epilepsy). I obtained the first evidence that neural dynamics maintains a distance to criticality, and thereby keep a safety margin to runaway activity. This distance to criticality is consistent across different species, but changes from wakefulness to deep sleep. Most recently, I succeeded in taming the out-of-equilibrium effects imposed by ongoing external stimulation, and as a result developed a very effective methods to quantify the distance to criticality from very little data. This opens novel avenues to study how networks tune their distance to criticality &#8211; and hence their information processing &#8211; from one moment to the other, depending on task requirement and context.</p>
<p>Selected References:<br />
Cramer et int., VP, <em>Nature Communications</em> (2020)<br />
VP et al., <em>Plos Comp. Biol.</em> (2013)<br />
Wilting &amp; VP, <em>Cerebral Cortex</em> (2019)<br />
Wilting &amp; VP, <em>Current Opinion Neurobiol.</em> (2019)<br />
Zierenberg, Wilting &amp; VP, <em>Physical Review X</em> (2018)</p>
<p><strong>III. Information Theory.</strong> Information theory is the natural language to quantify computation. I employ it in two complementary manners: <em>To infer the coding principles</em> of living neural networks, which were developed, tested and selected over million years of evolution, and as<em> guiding principle to design</em> <em>networks</em> for optimized processing.  Going a step further, we do not design the networks directly, but the <em>learning rules</em> that shape the network autonomously. In this realm, information theory provides the ideal language, because it is independent of semantics. Thereby it enables a comparison of coding principles across very diverse architectures, and implementations. This is useful when e.g. comparing coding across species and areas, and has been key for us to unravel a clear hierarchy of processing across cortex.  Our ongoing/future work goes into the role of phase transitions in information processing; local learning as a form of symmetry breaking; architectures to optimize Gibbs sampling despite strong correlations; and invariance of information flow under coarse-graining.</p>
<p>Selected References:<br />
Wibral, Lizier &amp; VP, <em>Frontiers in Robotics and AI</em> (2015)<br />
Wibral, Finn, Wollstadt, Lizier, VP, <em>Entropy</em> (2018)<br />
Wollstadt et al.,  <em>PLOS Computational Biology</em> (2017)</p>
<p><img class="wp-image-117 aligncenter" src="http://www.viola-priesemann.de/wp-content/uploads/2017/09/PartialInfoDecompDiagrams-ModifiedVsNonModified.bmp" alt="" width="364" height="353" /></p>
<p>&nbsp;</p>
<p><strong>IV. Inference and forecast of Disease Spread. </strong>With the outbreak of COVID-19, it turned out that our work on spreading dynamics in the brain under subsampling is highly valuable to investigate the spread of SARS-CoV-2.  We investigated how the reproduction number R changes as a consequence of a certain intervention like e.g. closing schools or imposing a contact ban. We then continued to investigate the challenges when mitigating COVID-19 spread via test-trace-isolate strategies. These strategies are inherently imperfect, and their capacity is limited. Hence, we identified an important tipping point: If case numbers are too high, so that a local health authority cannot test and trace the contacts anymore, and thus are too slow to break the infection chains, we see increasingly more unaware carriers. These unaware carriers are the drivers of a self-accelerating spread. The conclusion is straight forward: If case numbers are low, a control of the spread is much easier. Beyond the tipping point, regaining control becomes increasingly more difficult.<br />
In parallel, we are developing a hierarchical Bayesian model to investigate the impact of non-pharmaceutical interventions (NPIs) using European as well as federal data. And we critically revisit current case reports, and the effects of increased testing.<br />
Importantly, with great colleagues we jointly published a number of statements about COVID-19 mitigation and control strategies. To of these statements were  <a href="https://www.mpg.de/14760439/28-04-2020_Stellungnahme_Teil_01.pdf">signed by all four presidents,</a> of the Fraunhofer, Helmholtz, Leibnitz and Max Planck Society, and contributed to the statements of the Leopoldina.<br />
If you are interested in this line of research, feel welcome to join our team!</p>
<p>References:</p>
<p><a href="https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)32153-X/fulltext">Alwan et al., <em>The Lancet</em>, 2020</a><br />
<a href="https://arxiv.org/abs/2009.05732">Contreras et al., arxiv</a><br />
<a href="https://arxiv.org/abs/2004.01105">Dehning et al., <em>Science</em>, 2020</a><a href="https://arxiv.org/abs/2004.01105">/ arxiv</a><br />
<a href="https://www.medrxiv.org/content/10.1101/2020.09.16.20187484v1.abstract">Dehning et al., medrxiv</a><br />
<a href="https://arxiv.org/pdf/2010.05850">Linden et al., arxiv, accepted at Deutsches Ärzteblatt, 2020</a></p>
<p>Github: <a href="https://github.com/Priesemann-Group/covid19_inference_forecast">https://github.com/Priesemann-Group/covid19_inference_forecast</a></p>
<p><a href="https://www.mpg.de/14759871/corona-stellungnahme">Meyer-Hermmann*, Pigeot*, Priesemann*, Schöbel*:</a> Adaptive Strategien zur Eindämmung der COVID-19 Epidemie<br />
<a href="https://www.mpg.de/15426163/stellungnahme-ausseruniversitaere-forschungsorganisationen-covid-19-epidemie">Meyer-Hermmann*, Pigeot*, Priesemann*, Schöbel*:</a> Gemeinsam können wir es schaffen: Jeder einzelne Beitrag schützt Gesundheit, Gesellschaft und Wirtschaft</p>
<p>&nbsp;</p>
<p><strong>V. Music. </strong>Long-range correlations characterizes neural activity in the brain. Therefore music, which may be considered the mirror of the soul or the brain, should also reflect these correlations. In collaboration with Theo Geisel, we investigate how long range correlations and information-theoretic quantities change with genres, and whether long-range correlations play a central role in making Swing swing.<br />
References:<br />
Sogorski, Geisel &amp; VP, Plos ONE (2018)<br />
Datseris et al., arxiv / in press (2019)</p>
<figure id="attachment_115" style="width: 426px" class="wp-caption aligncenter"><a href="http://www.viola-priesemann.de/wp-content/uploads/2017/09/TS-Extraction-Paper.bmp"><img class="wp-image-115 size-full" src="http://www.viola-priesemann.de/wp-content/uploads/2017/09/TS-Extraction-Paper.bmp" alt="" width="426" height="276" /></a><figcaption class="wp-caption-text">To analyze long-range correlations in the tempo fluctuations of music performances, the beat of each piece must be extracted with millisecond precision [workflow from Sogorski, Geisel &amp; Priesemann, 2018].</figcaption></figure>
<p><strong>RECENT HIGHLIGHTS (10/2018)<br />
</strong></p>
<p>* Operating in a reverberating regime enables rapid tuning of network states to task requirements: <a href="https://www.frontiersin.org/articles/10.3389/fnsys.2018.00055/full">Wilting et al., <em>Front. Syst. Neurosci</em>., (2018)</a></p>
<p>* What makes in vitro and in vivo dynamics so different? &#8211; We derive the central role that the strength of external input plays in shaping collective dynamics: <a href="https://journals.aps.org/prx/abstract/10.1103/PhysRevX.8.031018">Zierenberg et al., <em>PRX</em> (2018)</a></p>
<p>* We developed two analytical solutions to overcome subsampling-induced bias,<br />
one for time series: <a href="https://arxiv.org/abs/1608.07035" target="_blank" rel="noopener">Wilting &amp; Priesemann, arXiv / <em>Nature Comm</em>. (2018)</a>,<br />
and one for spatial structures, such as degree distributions in a graph or avalanches: <a href="https://www.nature.com/articles/ncomms15140" target="_blank" rel="noopener">Levina &amp; Priesemann, <em>Nature Comm.</em> (2017)</a></p>


